# -*- coding: utf-8 -*-
"""Facedetect.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12X4se2zAC-_SmNZiw1FXV5-jbqnlGRl2
"""

#unpickling
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import keras
import tensorflow
import cv2
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.optimizers import Adam
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Load data
with open('/content/images.p','rb') as f:
  images = pickle.load(f)
with open('/content/labels.p','rb') as f:
  labels = pickle.load(f)

print("Original Data Shapes:", images.shape, labels.shape)

# Encode labels
le = LabelEncoder()
labels = le.fit_transform(labels)
n_persons = len(set(labels))
print("Number of persons:", n_persons)

# Print label mapping
l = le.inverse_transform(np.arange(n_persons))
for i in range(len(l)):
  print(i, '-->', l[i])

# Preprocessing function
def preprocessing(img):
  img = cv2.equalizeHist(img)
  img = img.reshape(100, 100, 1)
  img = img / 255.0
  return img

# Apply preprocessing
images = np.array(list(map(preprocessing, images)))
print("Shape of Input:", images.shape)

# Convert labels to categorical
labels = to_categorical(labels, num_classes=n_persons)

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)

# Data Augmentation to prevent overfitting
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    shear_range=0.1
)
datagen.fit(X_train)

def Lenet_Model():
  model = Sequential()
  model.add(Conv2D(32, (5, 5), input_shape=(100, 100, 1), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Conv2D(64, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Flatten())
  
  # Hidden layers with Dropout
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.5)) 
  model.add(Dense(64, activation='relu'))
  model.add(Dropout(0.3))
  
  # Output layer
  model.add(Dense(n_persons, activation='softmax'))
  
  model.compile(Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
  return model

model = Lenet_Model()
model.summary()

# Callbacks for better training
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001, verbose=1)

# Train the model
history = model.fit(datagen.flow(X_train, y_train, batch_size=32),
                    validation_data=(X_val, y_val),
                    epochs=15,
                    steps_per_epoch=14,
                    callbacks=[early_stopping, reduce_lr])

# Save the model
model.save("final_model.h5")
np.save("classes.npy", le.classes_)
print("Model and classes saved.")

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('Accuracy')
plt.show()
